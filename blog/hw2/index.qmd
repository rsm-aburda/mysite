---
title: "Poisson Regression Examples"

author: "Andrew Burda"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data


We begin by loading the dataset and performing exploratory analysis.

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load data
df = pd.read_csv("blueprinty.csv")
print(df)



#| echo: false
#| warning: false
#| message: false
sns.histplot(
    data=df,
    x="patents",
    hue="iscustomer",
    element="step",
    stat="count",
    binwidth=1,
    palette=["#1b9e77", "#d95f02"]
)
plt.title("Distribution of Patents by Customer Status")
plt.xlabel("Number of Patents Awarded")
plt.ylabel("Number of Firms")
plt.legend(title="Customer", labels=["Non-Customer", "Customer"])
plt.tight_layout()
plt.show()
#| echo: false
#| warning: false
#| message: false

# Summary statistics of patents by customer group
#| echo: false
#| warning: false
#| message: false
df.groupby("iscustomer")["patents"].agg(["mean", "median", "std", "count"]).rename(index={0: "Non-Customer", 1: "Customer"})
```
Next, we examine whether customer firms differ in age and regional location.  
Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
#| echo: false
#| warning: false
#| message: false
# Age distribution by customer status
sns.histplot(
    data=df,
    x="age",
    hue="iscustomer",
    element="step",
    stat="count",
    binwidth=5,
    palette=["#1b9e77", "#d95f02"]
)
plt.title("Firm Age by Customer Status")
plt.xlabel("Years Since Incorporation")
plt.ylabel("Number of Firms")
plt.legend(title="Customer", labels=["Non-Customer", "Customer"])
plt.tight_layout()
plt.show()
#| echo: false
#| warning: false
#| message: false
# Region distribution by customer status (proportion within each region)
pd.crosstab(df["region"], df["iscustomer"], normalize="index")\
  .rename(columns={0: "Non-Customer", 1: "Customer"})\
  .style.format("{:.1%}")

```

We observe that firms using Blueprinty tend to have more patents on average than those who do not. However, this raw difference may reflect underlying firm characteristics.




### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson distribution to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The probability mass function (pmf) of the Poisson distribution is:

$$
f(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Assuming independent observations across firms, the **likelihood** function for $n$ firms is:

$$
\mathcal{L}(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the log of the likelihood (to get the **log-likelihood**), we obtain:

$$
\log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log Y_i! \right)
$$

In the next step, we will numerically maximize this function to find the maximum likelihood estimate (MLE) of $\lambda$.


```{python}
#| echo: false
#| warning: false
#| message: false
import numpy as np
from scipy.optimize import minimize
from scipy.special import gammaln

# Extract Y values (number of patents)
y = df["patents"].values

# Define the negative log-likelihood function for the Poisson model
def neg_log_likelihood_poisson(lmbda):
    if lmbda[0] <= 0:
        return np.inf  # lambda must be positive
    ll = -np.sum(-lmbda[0] + y * np.log(lmbda[0]) - gammaln(y + 1))
    return ll

# Evaluate log-likelihood at an initial guess (e.g., lambda = 1)
neg_log_likelihood_poisson([1.0])

```
poisson_loglikelihood <- function(lambda, Y){
   ...
}


```{python}
#| echo: false
#| warning: false
#| message: false
# Create a range of lambda values to evaluate
lambda_vals = np.linspace(0.1, 10, 200)

# Calculate log-likelihood at each lambda
log_likelihoods = [-neg_log_likelihood_poisson([lmbda]) for lmbda in lambda_vals]

# Plot the log-likelihood curve
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, log_likelihoods, color="steelblue")
plt.title("Log-Likelihood of Poisson Model Across λ Values")
plt.xlabel("λ (Lambda)")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.tight_layout()
plt.show()
```

### Analytical MLE Derivation

To find the MLE analytically, we take the derivative of the log-likelihood:

$$
\log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log Y_i! \right)
$$

Take the derivative with respect to $\lambda$:

$$
\frac{d}{d\lambda} \log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i
$$

Set this equal to 0:

$$
-n + \frac{1}{\lambda} \sum Y_i = 0 \quad \Rightarrow \quad \lambda_{\text{MLE}} = \frac{1}{n} \sum Y_i = \bar{Y}
$$

Thus, the maximum likelihood estimator of $\lambda$ is simply the **sample mean** of the observed patent counts, which aligns with intuition: for a Poisson distribution, the mean and variance are both equal to $\lambda$.


```{python}
#| echo: false
#| warning: false
#| message: false
from scipy.optimize import minimize

# Initial guess for lambda
init_lambda = [1.0]

# Use scipy.optimize to minimize the negative log-likelihood
result = minimize(neg_log_likelihood_poisson, init_lambda, bounds=[(1e-6, None)])

# Extract estimated lambda
lambda_mle = result.x[0]
lambda_mle
```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i \sim \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the rate of patent awards varies by firm characteristics $X_i$.

We now update our log-likelihood function to be a function of a vector of regression coefficients $\beta$ and a covariate matrix $X$:

```{python}
#| echo: false
#| warning: false
#| message: false
import numpy as np
from scipy.special import gammaln

# Define the log-likelihood function for Poisson regression
def neg_log_likelihood_beta(beta, X, y):
    # Compute lambda_i = exp(X @ beta)
    lambda_i = np.exp(X @ beta)
    
    # Negative log-likelihood
    ll = -np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))
    return ll
```

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
```
```{python}
#| echo: false
#| warning: false
#| message: false
import patsy
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.special import gammaln

# Create squared age term (if not done already)
df["age_sq"] = df["age"] ** 2

# Design matrix with intercept, age, age_sq, region dummies (drop one), and customer indicator
X = patsy.dmatrix("1 + age + age_sq + C(region, Treatment) + iscustomer", data=df, return_type='dataframe')
y = df["patents"].values

# Define negative log-likelihood for Poisson regression
def neg_log_likelihood_beta(beta, X, y):
    lambda_i = np.exp(X @ beta)
    return -np.sum(-lambda_i + y * np.log(lambda_i + 1e-9) - gammaln(y + 1))

# Optimize log-likelihood
init_beta = np.zeros(X.shape[1])
result = minimize(neg_log_likelihood_beta, init_beta, args=(X.values, y), method="BFGS")

# Extract MLE and standard errors from inverse Hessian
beta_mle = result.x
cov_matrix = result.hess_inv
standard_errors = np.sqrt(np.diag(cov_matrix))

# Present table
coef_table = pd.DataFrame({
    "Coefficient": beta_mle,
    "Std. Error": standard_errors
}, index=X.design_info.column_names)

coef_table

```
```{python}
#| echo: false
#| warning: false
#| message: false
import statsmodels.api as sm

# Fit Poisson regression using statsmodels
glm_model = sm.GLM(y, X, family=sm.families.Poisson())
glm_results = glm_model.fit()

# Display summary table
glm_results.summary()
```
### Interpretation

The coefficient estimates from the Poisson regression using `statsmodels.GLM()` closely align with the MLE results we obtained using numerical optimization. This serves as a useful check and validates our earlier implementation.

**Key interpretation points:**

- **Intercept**: Represents the baseline log count of patents for a non-customer firm in the reference region (the region that was dropped in dummy encoding) with age and age² set to 0. While not directly interpretable on its own, it anchors the model.
- **Age & Age Squared**: The positive coefficient on `age` and the (typically) negative coefficient on `age_sq` suggest a concave relationship: the number of patents increases with age, but at a decreasing rate.
- **Customer Status (`iscustomer`)**: A positive and statistically significant coefficient on this variable implies that, holding age and region constant, Blueprinty customers tend to have more patents than non-customers. This supports the marketing team's hypothesis — but only after controlling for other variables.
- **Region Dummies**: These capture location-specific differences in patent productivity, relative to the omitted reference region.

Overall, the results suggest that firm age, region, and Blueprinty customer status are meaningful predictors of patent output.
```{python}
#| echo: false
#| warning: false
#| message: false
# Create counterfactual datasets
X_0 = X.copy()
X_0["iscustomer"] = 0

X_1 = X.copy()
X_1["iscustomer"] = 1

# Predict expected patent counts
y_pred_0 = glm_results.predict(X_0)
y_pred_1 = glm_results.predict(X_1)

# Estimate treatment effect
treatment_effect = (y_pred_1 - y_pred_0).mean()
treatment_effect
```

### Interpretation

To better understand the effect of Blueprinty's software on patent success, we constructed two counterfactual scenarios:

- `X_0`: all firms set as **non-customers** (`iscustomer = 0`)
- `X_1`: all firms set as **customers** (`iscustomer = 1`)

We used our fitted Poisson regression model to predict the number of patents for each firm under both scenarios. The difference in predicted patent counts reflects the **causal effect** of being a Blueprinty customer, holding all else constant.

We find that Blueprinty customers are predicted to receive, on average, **0.79 more patents** over a 5-year period compared to non-customers with similar characteristics. This supports the marketing team’s claim that the software improves patent success, even after adjusting for firm age and region.



## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::



### Exploratory Data Analysis

We explore the structure and key insights from the Airbnb dataset, focusing on listings in New York City.

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df_bnb = pd.read_csv("airbnb.csv")

# Convert binary field
df_bnb["instant_bookable"] = df_bnb["instant_bookable"].map({"t": 1, "f": 0})

# Drop missing values on relevant columns for EDA
cols = [
    "number_of_reviews", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location",
    "review_scores_value", "instant_bookable", "room_type"
]
df_eda = df_bnb.dropna(subset=cols)

#| echo: false
#| warning: false
#| message: false
sns.histplot(df_eda["number_of_reviews"], bins=50, color="#2a9d8f")
plt.title("Distribution of Number of Reviews")
plt.xlabel("Number of Reviews")
plt.ylabel("Listing Count")
plt.xlim(0, 100)
plt.tight_layout()
plt.show()


#| echo: false
#| warning: false
#| message: false
sns.histplot(df_eda[df_eda["price"] <= 500]["price"], bins=40, color="#e76f51")
plt.title("Distribution of Nightly Prices (Up to $500)")
plt.xlabel("Price ($)")
plt.ylabel("Listing Count")
plt.tight_layout()
plt.show()

#| echo: false
#| warning: false
#| message: false
room_counts = df_eda["room_type"].value_counts(normalize=True).rename_axis("Room Type").reset_index(name="Proportion")

sns.barplot(data=room_counts, x="Room Type", y="Proportion", palette="Set2")
plt.title("Room Type Proportions")
plt.ylim(0, 1)
plt.tight_layout()
plt.show()
#| echo: false
#| warning: false
#| message: false
sns.boxplot(x="review_scores_cleanliness", y="number_of_reviews", data=df_eda, palette="Blues")
plt.title("Reviews by Cleanliness Score")
plt.ylim(0, 100)
plt.xlabel("Cleanliness Score (1–10)")
plt.ylabel("Number of Reviews")
plt.tight_layout()
plt.show()



```

### Observations

- **Most listings are lightly reviewed**: The majority of listings have fewer than 20 reviews, indicating either new hosts or infrequent bookings. A small number of highly popular listings receive well over 100 reviews.
  
- **Prices are highly skewed**: Nightly rates are concentrated under $200, but the distribution is long-tailed, with a few luxury listings reaching $500 or more per night.

- **Room types vary in availability**: Entire homes and apartments dominate the market, followed by private rooms. Shared rooms are relatively rare.

- **Cleanliness matters**: Listings with higher cleanliness scores tend to attract more reviews, implying that guest satisfaction and perceived hygiene significantly influence engagement.

- **Instant booking is common**: While not visualized here, a large share of listings are instantly bookable, likely increasing conversion by reducing booking friction for guests.






### Differences in Age and Region by Customer Status

We examine whether Blueprinty customers differ systematically in age and region compared to non-customers. Since Blueprinty customers are not randomly assigned, this step helps assess potential selection bias.

```{python}
#| echo: false
#| warning: false
#| message: false
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Histogram of firm age by customer status
sns.histplot(data=df, x="age", hue="iscustomer", element="step", stat="density", binwidth=5, palette=["#1b9e77", "#d95f02"])
plt.title("Firm Age Distribution by Customer Status")
plt.xlabel("Years Since Incorporation")
plt.ylabel("Density")
plt.legend(title="Customer", labels=["Non-Customer", "Customer"])
plt.tight_layout()
plt.show()

#| echo: false
#| warning: false
#| message: false
# Crosstab of region by customer status (row-wise proportions)
region_compare = pd.crosstab(df["region"], df["iscustomer"], normalize="index")
region_compare.columns = ["Non-Customer", "Customer"]
region_compare = region_compare.round(2)
region_compare
```

### Observations

- **Firm Age**: Firms that use Blueprinty tend to be older than non-customer firms. This suggests that more established companies may be more likely to adopt Blueprinty’s software, possibly due to greater resources or a higher volume of patent activity.

- **Regional Differences**: Customer adoption is not evenly distributed across regions. Certain regions have a significantly higher proportion of Blueprinty users, indicating possible geographic or industry-driven differences in adoption behavior.

- **Implication**: These findings highlight potential selection bias. Since customer firms systematically differ from non-customers in both age and region, it's important to control for these variables in any causal analysis of Blueprinty's impact on patent success.

