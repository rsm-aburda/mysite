[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Homework 1\n\n\n\n\nAndrew Burda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nAndrew Burda\nApr 25, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/hw2/index.html",
    "href": "blog/hw2/index.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 1\nProf says make a chart."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Andrew’s Resume",
    "section": "",
    "text": "Last updated 2025-4-5\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Burda",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/hw1/index.html",
    "href": "blog/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe original experiment was conducted in partnership with a nonprofit that supported economically disadvantaged children. The researchers sent out 50,000 fundraising letters to potential donors, randomly assigning each recipient to one of three treatment groups:\n\nStandard Letter (Control): A basic appeal describing the mission of the organization and requesting support.\nMatching Grant Treatment: A letter stating that donations would be matched dollar-for-dollar by a lead donor, up to a specific amount.\nChallenge Grant Treatment: A letter explaining that a lead donor had pledged a large donation, but only if a threshold level of additional contributions was met.\n\nThe treatments were randomized to ensure internal validity, and the primary outcomes measured were: - A binary indicator of whether a donation was made - The dollar amount donated\nBy comparing outcomes across the three groups, Karlan and List aimed to uncover how different types of financial incentives (matching vs. challenge grants) influence donation behavior. The results showed that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants had a weaker and more ambiguous impact.\nThis experiment has since become a cornerstone in the field of behavioral economics and charitable giving, demonstrating how small shifts in message framing can meaningfully alter real-world behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/index.html#introduction",
    "href": "blog/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe original experiment was conducted in partnership with a nonprofit that supported economically disadvantaged children. The researchers sent out 50,000 fundraising letters to potential donors, randomly assigning each recipient to one of three treatment groups:\n\nStandard Letter (Control): A basic appeal describing the mission of the organization and requesting support.\nMatching Grant Treatment: A letter stating that donations would be matched dollar-for-dollar by a lead donor, up to a specific amount.\nChallenge Grant Treatment: A letter explaining that a lead donor had pledged a large donation, but only if a threshold level of additional contributions was met.\n\nThe treatments were randomized to ensure internal validity, and the primary outcomes measured were: - A binary indicator of whether a donation was made - The dollar amount donated\nBy comparing outcomes across the three groups, Karlan and List aimed to uncover how different types of financial incentives (matching vs. challenge grants) influence donation behavior. The results showed that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants had a weaker and more ambiguous impact.\nThis experiment has since become a cornerstone in the field of behavioral economics and charitable giving, demonstrating how small shifts in message framing can meaningfully alter real-world behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/index.html#data",
    "href": "blog/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset contains information used in Karlan & List (2007), including treatment indicators, match ratios, suggested donation amounts, and past giving behavior. It is used to evaluate the effect of matching donations in a fundraising experiment.\n\nimport pandas as pd\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\nprint(\"=== Data Overview ===\")\nprint(df.info())\n\nprint(\"\\n=== First 5 Rows ===\")\nprint(df.head())\n\nprint(\"\\n=== Descriptive Statistics ===\")\nprint(df.describe(include='all'))\n\n=== Data Overview ===\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\n=== First 5 Rows ===\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n=== Descriptive Statistics ===\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168560      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378105     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258633  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n::::\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport statsmodels.api as sm\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# T-test and regression for mrm2\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\nt_stat1, p_val1 = stats.ttest_ind(treat, control, nan_policy='omit')\n\ndf['intercept'] = 1\nmodel1 = sm.OLS(df['mrm2'], df[['intercept', 'treatment']], missing='drop').fit()\n\n# T-test and regression for amount\ntreat_amt = df[df['treatment'] == 1]['amount']\ncontrol_amt = df[df['treatment'] == 0]['amount']\nt_stat2, p_val2 = stats.ttest_ind(treat_amt, control_amt, nan_policy='omit')\n\nmodel2 = sm.OLS(df['amount'], df[['intercept', 'treatment']], missing='drop').fit()\n\n\nprint(\"=== mrm2 (Months Since Last Donation) ===\")\nprint(f\"T-test: T-stat = {t_stat1:.4f}, P = {p_val1:.4f}\")\nprint(model1.summary())\n\nprint(\"\\n=== amount (Previous Donation Amount) ===\")\nprint(f\"T-test: T-stat = {t_stat2:.4f}, P = {p_val2:.4f}\")\nprint(model2.summary())\n\n=== mrm2 (Months Since Last Donation) ===\nT-test: T-stat = 0.1195, P = 0.9049\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):              0.905\nTime:                        16:52:28   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== amount (Previous Donation Amount) ===\nT-test: T-stat = 1.8605, P = 0.0628\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        16:52:28   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThese results show that randomization appears to have worked. There are no statistically significant differences between the treatment and control groups on these baseline characteristics. This is important because it supports the internal validity of the experiment — we can reasonably believe that any differences in donation outcomes later on were caused by the treatment and not by pre-existing differences.\nThis is exactly why Table 1 is included in Karlan & List (2007) — to show that the treatment assignment was random and the groups were comparable at baseline."
  },
  {
    "objectID": "blog/hw1/index.html#experimental-results",
    "href": "blog/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n\ndf['donated'] = (df['amount'] &gt; 0).astype(int)\n\n\ndonation_rates = df.groupby('treatment')['donated'].mean()\n\n# Bar plot\nplt.bar(['Control', 'Treatment'], donation_rates)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Treatment Group')\nplt.ylim(0, 0.15)\nplt.grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\n\ntreat_d = df[df['treatment'] == 1]['donated']\ncontrol_d = df[df['treatment'] == 0]['donated']\nt_stat, p_val = stats.ttest_ind(treat_d, control_d)\nprint(f\"T-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n\nmodel = sm.OLS(df['donated'], df[['intercept', 'treatment']], missing='drop').fit()\nprint(model.summary())\n\nT-test: t = 3.1014, p = 0.0019\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                donated   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        16:52:29   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nprobit = sm.Probit(df['donated'], df[['intercept', 'treatment']], missing='drop').fit()\nprint(probit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                donated   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 25 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        16:52:29   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nTo see if matching donations increased the chance someone donated at all, I compared donation rates between the treatment and control groups.\nThe bar plot shows a clear difference: people in the treatment group donated at a higher rate than those in the control group.\nUsing both a t-test and a linear regression, I confirmed that this difference is statistically significant. This means the treatment group, who received a matching offer, was more likely to make any donation.\nI also ran a probit regression, which models the probability of making a donation. The results again showed that being assigned to the treatment group had a positive effect on whether someone donated.\nWhat we learn This tells us something important about human behavior: people are more likely to give when they know their donation will be matched. Even though the match doesn’t change their actual out-of-pocket cost, it creates a stronger sense of impact. That seems to motivate giving.\nIn the context of charitable giving, this suggests that match offers are an effective tool to increase participation, not just donation amounts. This aligns with the results shown in Table 2a Panel A and Table 3 of the paper. NOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndf['gave'] = (df['amount'] &gt; 0).astype(int)\n\n\ndf['ratio1'] = (df['ratio'] == '1:1').astype(int)\ndf['ratio2'] = (df['ratio'] == '2:1').astype(int)\ndf['ratio3'] = (df['ratio'] == '3:1').astype(int)\n\n\nfrom scipy import stats\n\n# Response rates by match ratio\nrate_1_1 = df[df['ratio'] == '1:1']['gave']\nrate_2_1 = df[df['ratio'] == '2:1']['gave']\nrate_3_1 = df[df['ratio'] == '3:1']['gave']\n\n# T-tests between groups\nt12, p12 = stats.ttest_ind(rate_1_1, rate_2_1, nan_policy='omit')\nt23, p23 = stats.ttest_ind(rate_2_1, rate_3_1, nan_policy='omit')\n\nprint(f\"1:1 vs 2:1 — T = {t12:.4f}, P = {p12:.4f}\")\nprint(f\"2:1 vs 3:1 — T = {t23:.4f}, P = {p23:.4f}\")\n\n1:1 vs 2:1 — T = nan, P = nan\n2:1 vs 3:1 — T = nan, P = nan\n\n\n\n# Add intercept\ndf['intercept'] = 1\n\n# Regression on dummy variables (1:1 is omitted)\nreg = sm.OLS(df['gave'], df[['intercept', 'ratio2', 'ratio3']], missing='drop').fit()\nprint(reg.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                       nan\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):                nan\nTime:                        16:52:29   Log-Likelihood:                 26625.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50082   BIC:                        -5.324e+04\nDf Model:                           0                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0206      0.001     32.493      0.000       0.019       0.022\nratio2              0          0        nan        nan           0           0\nratio3              0          0        nan        nan           0           0\n==============================================================================\nOmnibus:                    59825.030   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4320413.510\nSkew:                           6.742   Prob(JB):                         0.00\nKurtosis:                      46.457   Cond. No.                          inf\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is      0. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n/opt/conda/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1966: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\n\nmean_1_1 = rate_1_1.mean()\nmean_2_1 = rate_2_1.mean()\nmean_3_1 = rate_3_1.mean()\n\nprint(f\"Response rate (1:1): {mean_1_1:.4f}\")\nprint(f\"Response rate (2:1): {mean_2_1:.4f}\")\nprint(f\"Response rate (3:1): {mean_3_1:.4f}\")\nprint(f\"2:1 - 1:1 difference: {mean_2_1 - mean_1_1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {mean_3_1 - mean_2_1:.4f}\")\n\nResponse rate (1:1): nan\nResponse rate (2:1): nan\nResponse rate (3:1): nan\n2:1 - 1:1 difference: nan\n3:1 - 2:1 difference: nan\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Regression on all observations\nmodel_all = sm.OLS(df['amount'], df[['intercept', 'treatment']], missing='drop').fit()\nprint(model_all.summary())\n\n# T-test as a double check\nt_all, p_all = stats.ttest_ind(\n    df[df['treatment'] == 1]['amount'],\n    df[df['treatment'] == 0]['amount'],\n    nan_policy='omit'\n)\nprint(f\"T-test (all): t = {t_all:.4f}, p = {p_all:.4f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        16:52:29   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test (all): t = 1.8605, p = 0.0628\n\n\n\n# Subset to only donors\ndonors = df[df['amount'] &gt; 0]\n\n# Regression on donors only\nmodel_donors = sm.OLS(donors['amount'], donors[['intercept', 'treatment']], missing='drop').fit()\nprint(model_donors.summary())\n\n# Optional: t-test for donors only\nt_don, p_don = stats.ttest_ind(\n    donors[donors['treatment'] == 1]['amount'],\n    donors[donors['treatment'] == 0]['amount'],\n    nan_policy='omit'\n)\nprint(f\"T-test (donors only): t = {t_don:.4f}, p = {p_don:.4f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Fri, 25 Apr 2025   Prob (F-statistic):              0.561\nTime:                        16:52:30   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test (donors only): t = -0.5808, p = 0.5615\n\n\n\nimport matplotlib.pyplot as plt\n\n# Histograms by group\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\ncontrol_don = donors[donors['treatment'] == 0]['amount']\naxs[0].hist(control_don, bins=30, color='skyblue')\naxs[0].axvline(control_don.mean(), color='red', linestyle='dashed', linewidth=2)\naxs[0].set_title('Control Group Donations')\naxs[0].set_xlabel('Donation Amount')\naxs[0].set_ylabel('Frequency')\n\n# Treatment group\ntreat_don = donors[donors['treatment'] == 1]['amount']\naxs[1].hist(treat_don, bins=30, color='lightgreen')\naxs[1].axvline(treat_don.mean(), color='red', linestyle='dashed', linewidth=2)\naxs[1].set_title('Treatment Group Donations')\naxs[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/hw1/index.html#simulation-experiment",
    "href": "blog/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\n\n\nnp.random.seed(42)\n\n# Simulate 10,000 Bernoulli draws for control (p=0.018) and treatment (p=0.022)\ncontrol_draws = np.random.binomial(1, 0.018, 10000)\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\n\n# Calculate difference at each pair of draws\ndifferences = treatment_draws - control_draws\n\n# Compute cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color='navy')\nplt.axhline(0.004, color='red', linestyle='--', label='True Avg Difference (0.004)')\nplt.title(\"Cumulative Average Difference (LLN in Action)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTo show how the Law of Large Numbers (LLN) works, I simulated 10,000 random draws from each group’s true donation distribution:\nControl group had a 1.8% chance of donating (p = 0.018)\nTreatment group had a 2.2% chance (p = 0.022)\nAt each draw, I calculated the difference between treatment and control and then plotted the cumulative average of those differences.\nWhat we learn: Early in the plot, the average difference jumps around — this is due to random noise when sample sizes are small.\nBut as more data is added, the average settles down near the true expected treatment effect: 0.004.\nThis is the Law of Large Numbers in action: with more observations, our sample average converges to the true population average.\nThis helps explain why large sample sizes make our experimental estimates more reliable and less sensitive to random variation.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000\n\n\np_control = 0.018\np_treat = 0.022\n\n# Prepare plots\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(simulations):\n        c = np.random.binomial(1, p_control, n)\n        t = np.random.binomial(1, p_treat, n)\n        diffs.append(t.mean() - c.mean())\n\n    axs[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0.004, color='red', linestyle='--', label='True Avg Diff (0.004)')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg Difference (Treatment - Control)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo visualize the Central Limit Theorem (CLT), I created four histograms showing the sampling distribution of the average difference in donation rates between the treatment and control groups. For each sample size — 50, 200, 500, and 1000 — I simulated 1000 experiments and plotted the results.\nWhat we learn: With small samples (like 50), the distribution is wide and irregular — more influenced by random chance.\nAs the sample size increases, the distributions become tighter and more bell-shaped.\nBy the time we reach 1000 draws, the sampling distribution is narrow and symmetric, closely resembling a normal distribution centered near the true difference (0.004).\nThis shows the Central Limit Theorem in action: as the number of observations increases, the average of random variables (in this case, donation differences) becomes more predictable and normally distributed — even if the original data is binary.\nThis helps us understand why t-tests and confidence intervals work well in large-sample experiments — because the underlying averages behave in a stable, normal way."
  },
  {
    "objectID": "projects/Upcycled Supply Co/index.html",
    "href": "projects/Upcycled Supply Co/index.html",
    "title": "Andrew Burda",
    "section": "",
    "text": "title: “Upcycled Supply Co.” description: “A sustainable gear brand built from upcycled climbing rope and denim.” date: 2025-04-05 categories: [project, sustainability]"
  }
]