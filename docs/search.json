[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Homework 4\n\n\n \n\n\n\n\n\n\nAndrew Burda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n \n\n\n\n\n\nMay 11, 2025\nAndrew Burda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n \n\n\n\n\n\nApr 23, 2025\nAndrew Burda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3\n\n\n \n\n\n\n\n\n\nAndrew Burda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5\n\n\n \n\n\n\n\n\n\nAndrew Burda\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hw5/index.html",
    "href": "blog/hw5/index.html",
    "title": "Homework 5",
    "section": "",
    "text": "Question 1\nComing Soon"
  },
  {
    "objectID": "blog/hw1/index.html",
    "href": "blog/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe original experiment was conducted in partnership with a nonprofit that supported economically disadvantaged children. The researchers sent out 50,000 fundraising letters to potential donors, randomly assigning each recipient to one of three treatment groups:\n\nStandard Letter (Control): A basic appeal describing the mission of the organization and requesting support.\nMatching Grant Treatment: A letter stating that donations would be matched dollar-for-dollar by a lead donor, up to a specific amount.\nChallenge Grant Treatment: A letter explaining that a lead donor had pledged a large donation, but only if a threshold level of additional contributions was met.\n\nThe treatments were randomized to ensure internal validity, and the primary outcomes measured were: - A binary indicator of whether a donation was made - The dollar amount donated\nBy comparing outcomes across the three groups, Karlan and List aimed to uncover how different types of financial incentives (matching vs. challenge grants) influence donation behavior. The results showed that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants had a weaker and more ambiguous impact.\nThis experiment has since become a cornerstone in the field of behavioral economics and charitable giving, demonstrating how small shifts in message framing can meaningfully alter real-world behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/index.html#introduction",
    "href": "blog/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe original experiment was conducted in partnership with a nonprofit that supported economically disadvantaged children. The researchers sent out 50,000 fundraising letters to potential donors, randomly assigning each recipient to one of three treatment groups:\n\nStandard Letter (Control): A basic appeal describing the mission of the organization and requesting support.\nMatching Grant Treatment: A letter stating that donations would be matched dollar-for-dollar by a lead donor, up to a specific amount.\nChallenge Grant Treatment: A letter explaining that a lead donor had pledged a large donation, but only if a threshold level of additional contributions was met.\n\nThe treatments were randomized to ensure internal validity, and the primary outcomes measured were: - A binary indicator of whether a donation was made - The dollar amount donated\nBy comparing outcomes across the three groups, Karlan and List aimed to uncover how different types of financial incentives (matching vs. challenge grants) influence donation behavior. The results showed that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants had a weaker and more ambiguous impact.\nThis experiment has since become a cornerstone in the field of behavioral economics and charitable giving, demonstrating how small shifts in message framing can meaningfully alter real-world behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/index.html#data",
    "href": "blog/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset contains information used in Karlan & List (2007), including treatment indicators, match ratios, suggested donation amounts, and past giving behavior. It is used to evaluate the effect of matching donations in a fundraising experiment.\n\n\n=== Data Overview ===\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\n=== First 5 Rows ===\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n=== Descriptive Statistics ===\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168560      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378105     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258633  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n::::\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\n=== mrm2 (Months Since Last Donation) ===\nT-test: T-stat = 0.1195, P = 0.9049\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Sun, 11 May 2025   Prob (F-statistic):              0.905\nTime:                        15:35:33   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== amount (Previous Donation Amount) ===\nT-test: T-stat = 1.8605, P = 0.0628\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Sun, 11 May 2025   Prob (F-statistic):             0.0628\nTime:                        15:35:33   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThese results show that randomization appears to have worked. There are no statistically significant differences between the treatment and control groups on these baseline characteristics. This is important because it supports the internal validity of the experiment — we can reasonably believe that any differences in donation outcomes later on were caused by the treatment and not by pre-existing differences.\nThis is exactly why Table 1 is included in Karlan & List (2007) — to show that the treatment assignment was random and the groups were comparable at baseline."
  },
  {
    "objectID": "blog/hw1/index.html#experimental-results",
    "href": "blog/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n\n\nT-test: t = 3.1014, p = 0.0019\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                donated   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Sun, 11 May 2025   Prob (F-statistic):            0.00193\nTime:                        15:35:33   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                donated   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 11 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:35:34   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nTo see if matching donations increased the chance someone donated at all, I compared donation rates between the treatment and control groups.\nThe bar plot shows a clear difference: people in the treatment group donated at a higher rate than those in the control group.\nUsing both a t-test and a linear regression, I confirmed that this difference is statistically significant. This means the treatment group, who received a matching offer, was more likely to make any donation.\nI also ran a probit regression, which models the probability of making a donation. The results again showed that being assigned to the treatment group had a positive effect on whether someone donated.\nWhat we learn This tells us something important about human behavior: people are more likely to give when they know their donation will be matched. Even though the match doesn’t change their actual out-of-pocket cost, it creates a stronger sense of impact. That seems to motivate giving.\nIn the context of charitable giving, this suggests that match offers are an effective tool to increase participation, not just donation amounts. This aligns with the results shown in Table 2a Panel A and Table 3 of the paper. NOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n1:1 vs 2:1 — T = nan, P = nan\n2:1 vs 3:1 — T = nan, P = nan\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                       nan\nDate:                Sun, 11 May 2025   Prob (F-statistic):                nan\nTime:                        15:35:34   Log-Likelihood:                 26625.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50082   BIC:                        -5.324e+04\nDf Model:                           0                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0206      0.001     32.493      0.000       0.019       0.022\nratio2              0          0        nan        nan           0           0\nratio3              0          0        nan        nan           0           0\n==============================================================================\nOmnibus:                    59825.030   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4320413.510\nSkew:                           6.742   Prob(JB):                         0.00\nKurtosis:                      46.457   Cond. No.                          inf\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is      0. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n/opt/conda/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1966: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\n\n\nResponse rate (1:1): nan\nResponse rate (2:1): nan\nResponse rate (3:1): nan\n2:1 - 1:1 difference: nan\n3:1 - 2:1 difference: nan\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Sun, 11 May 2025   Prob (F-statistic):             0.0628\nTime:                        15:35:34   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test (all): t = 1.8605, p = 0.0628\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Sun, 11 May 2025   Prob (F-statistic):              0.561\nTime:                        15:35:34   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test (donors only): t = -0.5808, p = 0.5615"
  },
  {
    "objectID": "blog/hw1/index.html#simulation-experiment",
    "href": "blog/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nTo show how the Law of Large Numbers (LLN) works, I simulated 10,000 random draws from each group’s true donation distribution:\nControl group had a 1.8% chance of donating (p = 0.018)\nTreatment group had a 2.2% chance (p = 0.022)\nAt each draw, I calculated the difference between treatment and control and then plotted the cumulative average of those differences.\nWhat we learn: Early in the plot, the average difference jumps around — this is due to random noise when sample sizes are small.\nBut as more data is added, the average settles down near the true expected treatment effect: 0.004.\nThis is the Law of Large Numbers in action: with more observations, our sample average converges to the true population average.\nThis helps explain why large sample sizes make our experimental estimates more reliable and less sensitive to random variation.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nTo visualize the Central Limit Theorem (CLT), I created four histograms showing the sampling distribution of the average difference in donation rates between the treatment and control groups. For each sample size — 50, 200, 500, and 1000 — I simulated 1000 experiments and plotted the results.\nWhat we learn: With small samples (like 50), the distribution is wide and irregular — more influenced by random chance.\nAs the sample size increases, the distributions become tighter and more bell-shaped.\nBy the time we reach 1000 draws, the sampling distribution is narrow and symmetric, closely resembling a normal distribution centered near the true difference (0.004).\nThis shows the Central Limit Theorem in action: as the number of observations increases, the average of random variables (in this case, donation differences) becomes more predictable and normally distributed — even if the original data is binary.\nThis helps us understand why t-tests and confidence intervals work well in large-sample experiments — because the underlying averages behave in a stable, normal way."
  },
  {
    "objectID": "blog/hw4/index.html",
    "href": "blog/hw4/index.html",
    "title": "Homework 4",
    "section": "",
    "text": "Question 1\nComing Soon"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Andrew’s Resume",
    "section": "",
    "text": "Last updated 2024-4-5\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Andrew 👋",
    "section": "",
    "text": "Currently I’m a Data Analyst at GIA 💎 where I work on product analytics, experimentation, and roadmap development for internal and external platforms.\nCurrently, I’m pursuing my Master’s in Business Analytics at UC San Diego, where I’m deepening my skills in predictive modeling, statistical analysis, and decision science."
  },
  {
    "objectID": "blog/hw2/index.html",
    "href": "blog/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset and performing exploratory analysis.\n\n\n      patents     region   age  iscustomer\n0           0    Midwest  32.5           0\n1           3  Southwest  37.5           0\n2           4  Northwest  27.0           1\n3           3  Northeast  24.5           0\n4           3  Southwest  37.0           0\n...       ...        ...   ...         ...\n1495        2  Northeast  18.5           1\n1496        3  Southwest  22.5           0\n1497        4  Southwest  17.0           0\n1498        3      South  29.0           0\n1499        1      South  39.0           0\n\n[1500 rows x 4 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\niscustomer\n\n\n\n\n\n\n\n\nNon-Customer\n3.473013\n3.0\n2.225060\n1019\n\n\nCustomer\n4.133056\n4.0\n2.546846\n481\n\n\n\n\n\n\n\nNext, we examine whether customer firms differ in age and regional location.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customer\nCustomer\n\n\nregion\n \n \n\n\n\n\nMidwest\n83.5%\n16.5%\n\n\nNortheast\n45.4%\n54.6%\n\n\nNorthwest\n84.5%\n15.5%\n\n\nSouth\n81.7%\n18.3%\n\n\nSouthwest\n82.5%\n17.5%\n\n\n\n\n\nWe observe that firms using Blueprinty tend to have more patents on average than those who do not. However, this raw difference may reflect underlying firm characteristics.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson distribution to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function (pmf) of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations across firms, the likelihood function for \\(n\\) firms is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood (to get the log-likelihood), we obtain:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nIn the next step, we will numerically maximize this function to find the maximum likelihood estimate (MLE) of \\(\\lambda\\).\n\n\n6548.8869900694435\n\n\npoisson_loglikelihood &lt;- function(lambda, Y){ … }\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the MLE analytically, we take the derivative of the log-likelihood:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet this equal to 0:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of \\(\\lambda\\) is simply the sample mean of the observed patent counts, which aligns with intuition: for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the rate of patent awards varies by firm characteristics \\(X_i\\).\nWe now update our log-likelihood function to be a function of a vector of regression coefficients \\(\\beta\\) and a covariate matrix \\(X\\):\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.480059\n1.0\n\n\nC(region, Treatment)[T.Northeast]\n0.640979\n1.0\n\n\nC(region, Treatment)[T.Northwest]\n0.164288\n1.0\n\n\nC(region, Treatment)[T.South]\n0.181562\n1.0\n\n\nC(region, Treatment)[T.Southwest]\n0.295497\n1.0\n\n\nage\n38.016417\n1.0\n\n\nage_sq\n1033.539585\n1.0\n\n\niscustomer\n0.553874\n1.0\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nSun, 11 May 2025\nDeviance:\n2143.3\n\n\nTime:\n15:35:28\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nC(region, Treatment)[T.Northeast]\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nC(region, Treatment)[T.Northwest]\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nC(region, Treatment)[T.South]\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nC(region, Treatment)[T.Southwest]\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_sq\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\n\n\n\nThe coefficient estimates from the Poisson regression using statsmodels.GLM() closely align with the MLE results we obtained using numerical optimization. This serves as a useful check and validates our earlier implementation.\nKey interpretation points:\n\nIntercept: Represents the baseline log count of patents for a non-customer firm in the reference region (the region that was dropped in dummy encoding) with age and age² set to 0. While not directly interpretable on its own, it anchors the model.\nAge & Age Squared: The positive coefficient on age and the (typically) negative coefficient on age_sq suggest a concave relationship: the number of patents increases with age, but at a decreasing rate.\nCustomer Status (iscustomer): A positive and statistically significant coefficient on this variable implies that, holding age and region constant, Blueprinty customers tend to have more patents than non-customers. This supports the marketing team’s hypothesis — but only after controlling for other variables.\nRegion Dummies: These capture location-specific differences in patent productivity, relative to the omitted reference region.\n\nOverall, the results suggest that firm age, region, and Blueprinty customer status are meaningful predictors of patent output.\n\n\n0.7927680710452915\n\n\n\n\n\nTo better understand the effect of Blueprinty’s software on patent success, we constructed two counterfactual scenarios:\n\nX_0: all firms set as non-customers (iscustomer = 0)\nX_1: all firms set as customers (iscustomer = 1)\n\nWe used our fitted Poisson regression model to predict the number of patents for each firm under both scenarios. The difference in predicted patent counts reflects the causal effect of being a Blueprinty customer, holding all else constant.\nWe find that Blueprinty customers are predicted to receive, on average, 0.79 more patents over a 5-year period compared to non-customers with similar characteristics. This supports the marketing team’s claim that the software improves patent success, even after adjusting for firm age and region."
  },
  {
    "objectID": "blog/hw2/index.html#blueprinty-case-study",
    "href": "blog/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset and performing exploratory analysis.\n\n\n      patents     region   age  iscustomer\n0           0    Midwest  32.5           0\n1           3  Southwest  37.5           0\n2           4  Northwest  27.0           1\n3           3  Northeast  24.5           0\n4           3  Southwest  37.0           0\n...       ...        ...   ...         ...\n1495        2  Northeast  18.5           1\n1496        3  Southwest  22.5           0\n1497        4  Southwest  17.0           0\n1498        3      South  29.0           0\n1499        1      South  39.0           0\n\n[1500 rows x 4 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\niscustomer\n\n\n\n\n\n\n\n\nNon-Customer\n3.473013\n3.0\n2.225060\n1019\n\n\nCustomer\n4.133056\n4.0\n2.546846\n481\n\n\n\n\n\n\n\nNext, we examine whether customer firms differ in age and regional location.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customer\nCustomer\n\n\nregion\n \n \n\n\n\n\nMidwest\n83.5%\n16.5%\n\n\nNortheast\n45.4%\n54.6%\n\n\nNorthwest\n84.5%\n15.5%\n\n\nSouth\n81.7%\n18.3%\n\n\nSouthwest\n82.5%\n17.5%\n\n\n\n\n\nWe observe that firms using Blueprinty tend to have more patents on average than those who do not. However, this raw difference may reflect underlying firm characteristics.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson distribution to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function (pmf) of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations across firms, the likelihood function for \\(n\\) firms is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood (to get the log-likelihood), we obtain:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nIn the next step, we will numerically maximize this function to find the maximum likelihood estimate (MLE) of \\(\\lambda\\).\n\n\n6548.8869900694435\n\n\npoisson_loglikelihood &lt;- function(lambda, Y){ … }\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the MLE analytically, we take the derivative of the log-likelihood:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet this equal to 0:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator of \\(\\lambda\\) is simply the sample mean of the observed patent counts, which aligns with intuition: for a Poisson distribution, the mean and variance are both equal to \\(\\lambda\\).\n\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the rate of patent awards varies by firm characteristics \\(X_i\\).\nWe now update our log-likelihood function to be a function of a vector of regression coefficients \\(\\beta\\) and a covariate matrix \\(X\\):\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.480059\n1.0\n\n\nC(region, Treatment)[T.Northeast]\n0.640979\n1.0\n\n\nC(region, Treatment)[T.Northwest]\n0.164288\n1.0\n\n\nC(region, Treatment)[T.South]\n0.181562\n1.0\n\n\nC(region, Treatment)[T.Southwest]\n0.295497\n1.0\n\n\nage\n38.016417\n1.0\n\n\nage_sq\n1033.539585\n1.0\n\n\niscustomer\n0.553874\n1.0\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nSun, 11 May 2025\nDeviance:\n2143.3\n\n\nTime:\n15:35:28\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nC(region, Treatment)[T.Northeast]\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nC(region, Treatment)[T.Northwest]\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nC(region, Treatment)[T.South]\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nC(region, Treatment)[T.Southwest]\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_sq\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\n\n\n\nThe coefficient estimates from the Poisson regression using statsmodels.GLM() closely align with the MLE results we obtained using numerical optimization. This serves as a useful check and validates our earlier implementation.\nKey interpretation points:\n\nIntercept: Represents the baseline log count of patents for a non-customer firm in the reference region (the region that was dropped in dummy encoding) with age and age² set to 0. While not directly interpretable on its own, it anchors the model.\nAge & Age Squared: The positive coefficient on age and the (typically) negative coefficient on age_sq suggest a concave relationship: the number of patents increases with age, but at a decreasing rate.\nCustomer Status (iscustomer): A positive and statistically significant coefficient on this variable implies that, holding age and region constant, Blueprinty customers tend to have more patents than non-customers. This supports the marketing team’s hypothesis — but only after controlling for other variables.\nRegion Dummies: These capture location-specific differences in patent productivity, relative to the omitted reference region.\n\nOverall, the results suggest that firm age, region, and Blueprinty customer status are meaningful predictors of patent output.\n\n\n0.7927680710452915\n\n\n\n\n\nTo better understand the effect of Blueprinty’s software on patent success, we constructed two counterfactual scenarios:\n\nX_0: all firms set as non-customers (iscustomer = 0)\nX_1: all firms set as customers (iscustomer = 1)\n\nWe used our fitted Poisson regression model to predict the number of patents for each firm under both scenarios. The difference in predicted patent counts reflects the causal effect of being a Blueprinty customer, holding all else constant.\nWe find that Blueprinty customers are predicted to receive, on average, 0.79 more patents over a 5-year period compared to non-customers with similar characteristics. This supports the marketing team’s claim that the software improves patent success, even after adjusting for firm age and region."
  },
  {
    "objectID": "blog/hw2/index.html#airbnb-case-study",
    "href": "blog/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\nWe explore the structure and key insights from the Airbnb dataset, focusing on listings in New York City.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\nMost listings are lightly reviewed: The majority of listings have fewer than 20 reviews, indicating either new hosts or infrequent bookings. A small number of highly popular listings receive well over 100 reviews.\nPrices are highly skewed: Nightly rates are concentrated under $200, but the distribution is long-tailed, with a few luxury listings reaching $500 or more per night.\nRoom types vary in availability: Entire homes and apartments dominate the market, followed by private rooms. Shared rooms are relatively rare.\nCleanliness matters: Listings with higher cleanliness scores tend to attract more reviews, implying that guest satisfaction and perceived hygiene significantly influence engagement.\nInstant booking is common: While not visualized here, a large share of listings are instantly bookable, likely increasing conversion by reducing booking friction for guests.\n\n\n\nDifferences in Age and Region by Customer Status\nWe examine whether Blueprinty customers differ systematically in age and region compared to non-customers. Since Blueprinty customers are not randomly assigned, this step helps assess potential selection bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Customer\nCustomer\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.83\n0.17\n\n\nNortheast\n0.45\n0.55\n\n\nNorthwest\n0.84\n0.16\n\n\nSouth\n0.82\n0.18\n\n\nSouthwest\n0.82\n0.18\n\n\n\n\n\n\n\n\n\nObservations\n\nFirm Age: Firms that use Blueprinty tend to be older than non-customer firms. This suggests that more established companies may be more likely to adopt Blueprinty’s software, possibly due to greater resources or a higher volume of patent activity.\nRegional Differences: Customer adoption is not evenly distributed across regions. Certain regions have a significantly higher proportion of Blueprinty users, indicating possible geographic or industry-driven differences in adoption behavior.\nImplication: These findings highlight potential selection bias. Since customer firms systematically differ from non-customers in both age and region, it’s important to control for these variables in any causal analysis of Blueprinty’s impact on patent success."
  },
  {
    "objectID": "blog/hw3/index.html",
    "href": "blog/hw3/index.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 1\nComing Soon"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  }
]